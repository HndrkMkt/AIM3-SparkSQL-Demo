{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AIM-3 Spark SQL Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start a SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"AIM-3 Spark SQL Demo\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data from CSVs (could also use Hive, Parquet,...) and cache it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Project ID: string (nullable = true)\n",
      " |-- Donation ID: string (nullable = true)\n",
      " |-- Donor ID: string (nullable = true)\n",
      " |-- Donation Included Optional Donation: string (nullable = true)\n",
      " |-- Donation Amount: double (nullable = true)\n",
      " |-- Donor Cart Sequence: integer (nullable = true)\n",
      " |-- Donation Received Date: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "donations = spark.read.csv('data/Donations.csv', mode = \"DROPMALFORMED\", inferSchema = True, header = True).cache()\n",
    "donations.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Donor ID: string (nullable = true)\n",
      " |-- Donor City: string (nullable = true)\n",
      " |-- Donor State: string (nullable = true)\n",
      " |-- Donor Is Teacher: string (nullable = true)\n",
      " |-- Donor Zip: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "donors = spark.read.csv('data/Donors.csv', mode = \"DROPMALFORMED\", inferSchema = True, header = True).cache()\n",
    "donors.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+-----------------------------------+---------------+-------------------+----------------------+\n",
      "|          Project ID|         Donation ID|            Donor ID|Donation Included Optional Donation|Donation Amount|Donor Cart Sequence|Donation Received Date|\n",
      "+--------------------+--------------------+--------------------+-----------------------------------+---------------+-------------------+----------------------+\n",
      "|000009891526c0ade...|68872912085866622...|1f4b5b6e68445c6c4...|                                 No|         178.37|                 11|   2016-08-23 13:15:57|\n",
      "|000009891526c0ade...|dcf1071da3aa3561f...|4aaab6d244bf35996...|                                Yes|           25.0|                  2|   2016-06-06 20:05:23|\n",
      "|000009891526c0ade...|18a234b9d1e538c43...|0b0765dc9c759adc4...|                                Yes|           20.0|                  3|   2016-06-06 14:08:46|\n",
      "|000009891526c0ade...|38d2744bf9138b0b5...|377944ad61f72d800...|                                Yes|           25.0|                  1|   2016-05-15 10:23:04|\n",
      "|000009891526c0ade...|5a032791e31167a70...|6d5b22d39e68c6560...|                                Yes|           25.0|                  2|   2016-05-17 01:23:38|\n",
      "+--------------------+--------------------+--------------------+-----------------------------------+---------------+-------------------+----------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "donations.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register DataFrames as tables to use them in SQL queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "donations.createOrReplaceTempView(\"donations\")\n",
    "donors.createOrReplaceTempView(\"donors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze data using SQL queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|  Total Donations|\n",
      "+-----------------+\n",
      "|7343436.779999997|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = spark.sql(\"\"\"\n",
    "SELECT sum(`Donation Amount`) AS `Total Donations`\n",
    "FROM donations \n",
    "WHERE `Donation Received Date` BETWEEN '2016-09-01' AND '2016-09-30'\n",
    "\"\"\")\n",
    "# Alternatively, use DataFrames API to get results\n",
    "# results = donations \\\n",
    "#.where(\"`Donation Received Date` BETWEEN '2016-09-01' AND '2016-09-30'\") \\\n",
    "#.agg(sum(\"Donation Amount\"))\n",
    "results.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------------------+\n",
      "|   Donor State|   Total Donations|\n",
      "+--------------+------------------+\n",
      "|    California|        4351350.57|\n",
      "|      New York|2300683.9300000006|\n",
      "|         Texas|1593148.5500000007|\n",
      "|      Illinois|1434985.6100000003|\n",
      "|       Florida|1183645.6500000004|\n",
      "|North Carolina|1120992.6000000003|\n",
      "| Massachusetts|1007399.6199999998|\n",
      "|    Washington|         808259.33|\n",
      "|  Pennsylvania| 799274.9200000005|\n",
      "|       Georgia| 694610.0200000006|\n",
      "+--------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "donations_by_state = spark.sql(\"\"\"\n",
    "SELECT `Donor State`,\n",
    "sum(`Donation Amount`) AS `Total Donations`\n",
    "FROM donations \n",
    "JOIN donors ON donations.`Donor ID` == donors.`Donor ID`\n",
    "WHERE `Donation Received Date` BETWEEN '2016-09-01' AND '2016-12-31'\n",
    "GROUP BY `Donor State`\n",
    "ORDER BY 2 DESC\n",
    "\"\"\")\n",
    "donations_by_state.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use a  lambda function as a User Defined Function in SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------------------+\n",
      "|   Donor State|Total Donations in Euro|\n",
      "+--------------+-----------------------+\n",
      "|    California|     3742161.4902000003|\n",
      "|      New York|     1978588.1798000005|\n",
      "|         Texas|     1370107.7530000007|\n",
      "|      Illinois|     1234087.6246000002|\n",
      "|       Florida|     1017935.2590000003|\n",
      "|North Carolina|      964053.6360000003|\n",
      "| Massachusetts|      866363.6731999997|\n",
      "|    Washington|            695103.0238|\n",
      "|  Pennsylvania|      687376.4312000005|\n",
      "|       Georgia|      597364.6172000006|\n",
      "+--------------+-----------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.udf.register(\"DtoEU\", lambda dollars: (dollars * 0.86))\n",
    "donations_by_state.createOrReplaceTempView(\"donations_by_state\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "SELECT `Donor State`,\n",
    "DtoEU(`Total Donations`) AS `Total Donations in Euro`\n",
    "FROM donations_by_state\"\"\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSVs can also be saved as partitioned Hive tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "donations.select(col(\"Project ID\").alias(\"project_id\"), \n",
    "                 col(\"Donation ID\").alias(\"donation_id\"),\n",
    "                 col(\"Donor ID\").alias(\"donor_id\"),\n",
    "                 col(\"Donation Included Optional Donation\").alias(\"donation_included_optional_donation\"),\n",
    "                 col(\"Donation Amount\").alias(\"donation_amount\"),\n",
    "                 col(\"Donor Cart Sequence\").alias(\"donor_cart_sequence\"),\n",
    "                 col(\"Donation Received Date\").alias(\"donation_received_date\")) \\\n",
    ".write.bucketBy(10, \"donation_received_date\").saveAsTable(\"donations_hive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "donors.select(col(\"Donor ID\").alias(\"donor_id\"),\n",
    "              col(\"Donor City\").alias(\"donor_city\"),\n",
    "              col(\"Donor State\").alias(\"donor_state\"),\n",
    "              col(\"Donor Is Teacher\").alias(\"donor_is_teacher\"),\n",
    "              col(\"Donor Zip\").alias(\"donor_zip\")) \\\n",
    ".write.partitionBy(\"donor_state\").saveAsTable(\"donors_hive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------------------+\n",
      "|   donor_state|   Total Donations|\n",
      "+--------------+------------------+\n",
      "|    California| 4351350.569999999|\n",
      "|      New York|2300683.9300000006|\n",
      "|         Texas|1593148.5500000007|\n",
      "|      Illinois|1434985.6100000003|\n",
      "|       Florida|1183645.6500000006|\n",
      "|North Carolina|1120992.6000000003|\n",
      "| Massachusetts|        1007399.62|\n",
      "|    Washington| 808259.3299999998|\n",
      "|  Pennsylvania| 799274.9200000005|\n",
      "|       Georgia| 694610.0200000006|\n",
      "+--------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "donations_by_state = spark.sql(\"\"\"\n",
    "SELECT `donor_state`,\n",
    "sum(`donation_amount`) AS `Total Donations`\n",
    "FROM donations_hive \n",
    "JOIN donors_hive ON donations_hive.donor_id == donors_hive.donor_id\n",
    "WHERE donation_received_date BETWEEN '2016-09-01' AND '2016-12-31'\n",
    "GROUP BY donor_state\n",
    "ORDER BY 2 DESC\n",
    "\"\"\")\n",
    "donations_by_state.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hive tables can be analyzed to collect statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"ANALYZE TABLE donations_hive COMPUTE STATISTICS\"\"\");\n",
    "spark.sql(\"\"\"ANALYZE TABLE donors_hive COMPUTE STATISTICS\"\"\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use EXPLAIN to see query optimization at work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Project ['sum('donation_amount) AS Total Donations#523]\n",
      "+- 'Filter (('donation_received_date >= 2016-09-01) && ('donation_received_date <= 2016-09-30))\n",
      "   +- 'UnresolvedRelation `donations_hive`\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "Total Donations: double\n",
      "Aggregate [sum(donation_amount#528) AS Total Donations#523]\n",
      "+- Filter ((cast(donation_received_date#530 as string) >= 2016-09-01) && (cast(donation_received_date#530 as string) <= 2016-09-30))\n",
      "   +- SubqueryAlias donations_hive\n",
      "      +- Relation[project_id#524,donation_id#525,donor_id#526,donation_included_optional_donation#527,donation_amount#528,donor_cart_sequence#529,donation_received_date#530] parquet\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Aggregate [sum(donation_amount#528) AS Total Donations#523]\n",
      "+- Project [donation_amount#528]\n",
      "   +- Filter ((isnotnull(donation_received_date#530) && (cast(donation_received_date#530 as string) >= 2016-09-01)) && (cast(donation_received_date#530 as string) <= 2016-09-30))\n",
      "      +- Relation[project_id#524,donation_id#525,donor_id#526,donation_included_optional_donation#527,donation_amount#528,donor_cart_sequence#529,donation_received_date#530] parquet\n",
      "\n",
      "== Physical Plan ==\n",
      "*(2) HashAggregate(keys=[], functions=[sum(donation_amount#528)], output=[Total Donations#523])\n",
      "+- Exchange SinglePartition\n",
      "   +- *(1) HashAggregate(keys=[], functions=[partial_sum(donation_amount#528)], output=[sum#534])\n",
      "      +- *(1) Project [donation_amount#528]\n",
      "         +- *(1) Filter ((isnotnull(donation_received_date#530) && (cast(donation_received_date#530 as string) >= 2016-09-01)) && (cast(donation_received_date#530 as string) <= 2016-09-30))\n",
      "            +- *(1) FileScan parquet default.donations_hive[donation_amount#528,donation_received_date#530] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/home/hendrik/projects/tu_berlin/aim3/demo/spark-warehouse/donations_hive], PartitionFilters: [], PushedFilters: [IsNotNull(donation_received_date)], ReadSchema: struct<donation_amount:double,donation_received_date:timestamp>\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT sum(`donation_amount`) AS `Total Donations`\n",
    "FROM donations_hive \n",
    "WHERE `donation_received_date` BETWEEN '2016-09-01' AND '2016-09-30'\n",
    "\"\"\").explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Aggregate ['Donor State], ['Donor State, 'sum('Donation Amount) AS Total Donations#535]\n",
      "+- 'Filter (('Donation Received Date >= 2016-09-01) && ('Donation Received Date <= 2016-12-31))\n",
      "   +- 'Join Inner, ('donations.Donor ID = 'donors.Donor ID)\n",
      "      :- 'UnresolvedRelation `donations`\n",
      "      +- 'UnresolvedRelation `donors`\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "Donor State: string, Total Donations: double\n",
      "Aggregate [Donor State#78], [Donor State#78, sum(Donation Amount#14) AS Total Donations#535]\n",
      "+- Filter ((cast(Donation Received Date#16 as string) >= 2016-09-01) && (cast(Donation Received Date#16 as string) <= 2016-12-31))\n",
      "   +- Join Inner, (Donor ID#12 = Donor ID#76)\n",
      "      :- SubqueryAlias donations\n",
      "      :  +- Relation[Project ID#10,Donation ID#11,Donor ID#12,Donation Included Optional Donation#13,Donation Amount#14,Donor Cart Sequence#15,Donation Received Date#16] csv\n",
      "      +- SubqueryAlias donors\n",
      "         +- Relation[Donor ID#76,Donor City#77,Donor State#78,Donor Is Teacher#79,Donor Zip#80] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Aggregate [Donor State#78], [Donor State#78, sum(Donation Amount#14) AS Total Donations#535]\n",
      "+- Project [Donation Amount#14, Donor State#78]\n",
      "   +- Join Inner, (Donor ID#12 = Donor ID#76)\n",
      "      :- Project [Donor ID#12, Donation Amount#14]\n",
      "      :  +- Filter (((isnotnull(Donation Received Date#16) && (cast(Donation Received Date#16 as string) >= 2016-09-01)) && (cast(Donation Received Date#16 as string) <= 2016-12-31)) && isnotnull(Donor ID#12))\n",
      "      :     +- InMemoryRelation [Project ID#10, Donation ID#11, Donor ID#12, Donation Included Optional Donation#13, Donation Amount#14, Donor Cart Sequence#15, Donation Received Date#16], true, 10000, StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "      :           +- *(1) FileScan csv [Project ID#10,Donation ID#11,Donor ID#12,Donation Included Optional Donation#13,Donation Amount#14,Donor Cart Sequence#15,Donation Received Date#16] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/home/hendrik/projects/tu_berlin/aim3/demo/data/Donations.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<Project ID:string,Donation ID:string,Donor ID:string,Donation Included Optional Donation:s...\n",
      "      +- Project [Donor ID#76, Donor State#78]\n",
      "         +- Filter isnotnull(Donor ID#76)\n",
      "            +- InMemoryRelation [Donor ID#76, Donor City#77, Donor State#78, Donor Is Teacher#79, Donor Zip#80], true, 10000, StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "                  +- *(1) FileScan csv [Donor ID#76,Donor City#77,Donor State#78,Donor Is Teacher#79,Donor Zip#80] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/home/hendrik/projects/tu_berlin/aim3/demo/data/Donors.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<Donor ID:string,Donor City:string,Donor State:string,Donor Is Teacher:string,Donor Zip:str...\n",
      "\n",
      "== Physical Plan ==\n",
      "*(6) HashAggregate(keys=[Donor State#78], functions=[sum(Donation Amount#14)], output=[Donor State#78, Total Donations#535])\n",
      "+- Exchange hashpartitioning(Donor State#78, 200)\n",
      "   +- *(5) HashAggregate(keys=[Donor State#78], functions=[partial_sum(Donation Amount#14)], output=[Donor State#78, sum#600])\n",
      "      +- *(5) Project [Donation Amount#14, Donor State#78]\n",
      "         +- *(5) SortMergeJoin [Donor ID#12], [Donor ID#76], Inner\n",
      "            :- *(2) Sort [Donor ID#12 ASC NULLS FIRST], false, 0\n",
      "            :  +- Exchange hashpartitioning(Donor ID#12, 200)\n",
      "            :     +- *(1) Project [Donor ID#12, Donation Amount#14]\n",
      "            :        +- *(1) Filter (((isnotnull(Donation Received Date#16) && (cast(Donation Received Date#16 as string) >= 2016-09-01)) && (cast(Donation Received Date#16 as string) <= 2016-12-31)) && isnotnull(Donor ID#12))\n",
      "            :           +- InMemoryTableScan [Donation Amount#14, Donation Received Date#16, Donor ID#12], [isnotnull(Donation Received Date#16), (cast(Donation Received Date#16 as string) >= 2016-09-01), (cast(Donation Received Date#16 as string) <= 2016-12-31), isnotnull(Donor ID#12)]\n",
      "            :                 +- InMemoryRelation [Project ID#10, Donation ID#11, Donor ID#12, Donation Included Optional Donation#13, Donation Amount#14, Donor Cart Sequence#15, Donation Received Date#16], true, 10000, StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "            :                       +- *(1) FileScan csv [Project ID#10,Donation ID#11,Donor ID#12,Donation Included Optional Donation#13,Donation Amount#14,Donor Cart Sequence#15,Donation Received Date#16] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/home/hendrik/projects/tu_berlin/aim3/demo/data/Donations.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<Project ID:string,Donation ID:string,Donor ID:string,Donation Included Optional Donation:s...\n",
      "            +- *(4) Sort [Donor ID#76 ASC NULLS FIRST], false, 0\n",
      "               +- Exchange hashpartitioning(Donor ID#76, 200)\n",
      "                  +- *(3) Filter isnotnull(Donor ID#76)\n",
      "                     +- InMemoryTableScan [Donor ID#76, Donor State#78], [isnotnull(Donor ID#76)]\n",
      "                           +- InMemoryRelation [Donor ID#76, Donor City#77, Donor State#78, Donor Is Teacher#79, Donor Zip#80], true, 10000, StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "                                 +- *(1) FileScan csv [Donor ID#76,Donor City#77,Donor State#78,Donor Is Teacher#79,Donor Zip#80] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/home/hendrik/projects/tu_berlin/aim3/demo/data/Donors.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<Donor ID:string,Donor City:string,Donor State:string,Donor Is Teacher:string,Donor Zip:str...\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT `Donor State`,\n",
    "sum(`Donation Amount`) AS `Total Donations`\n",
    "FROM donations \n",
    "JOIN donors ON donations.`Donor ID` == donors.`Donor ID`\n",
    "WHERE `Donation Received Date` BETWEEN '2016-09-01' AND '2016-12-31'\n",
    "GROUP BY `Donor State`\n",
    "\"\"\").explain(True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
